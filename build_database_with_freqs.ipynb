{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [1. Load up libraries](#1.-Load-up-libraries)\n",
    "    * [1.1 Define functions](#1.1-Define-functions)\n",
    "* [2. Rebuild database](#2.-Rebuild-database)\n",
    "    * [2.1 Load data from AWS SQL database](#2.1-Load-data-from-AWS-SQL-database) \n",
    "    * [2.2 Build 'freq' and 'normed_pred_diff' columns](#2.2-Build-'freq'-and-'normed_pred_diff'-columns)\n",
    "        * [2.2.1 Write to database](#2.2.1-Write-to-database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load up libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "# all purpose\n",
    "import datetime, re\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "# for talking to SQL databases\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "\n",
    "# all purpose data analysis and plotting\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# for ML\n",
    "import patsy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import cross_validation, metrics, linear_model, svm\n",
    "# needed for cross-validation on sets where the test data is not binary/multiclass \n",
    "# (i.e. needed for regressors, not classifiers)\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# for saving output\n",
    "import pickle\n",
    "\n",
    "# load up Muni routes\n",
    "list_of_muni_routes = np.load('/Users/dstone/Dropbox/insight/project/busUnBunchr_site/list_of_muni_routes.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Define functions\n",
    "\n",
    "Function that gets frequency for specified route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_frequency_for_route(pair_route, pair_time):\n",
    "    ''' Must past pair_time as a time stamp \n",
    "        (which should come from dataframe that you are adding to) \n",
    "    '''\n",
    "    with open('/Users/dstone/Dropbox/insight/project/busUnBunchr_site/route_frequencies/route_'+str(pair_route)+'_frequencies.pkl','rb') as input:\n",
    "        freq_df  = pickle.load(input)\n",
    "    t0 = '1970-01-01 '+str(pair_time.hour)+':'+str(pair_time.minute)+':00'\n",
    "    # set to end of day, no messing around\n",
    "    t1 = '1970-01-01 23:59:00'\n",
    "    # some more error catching\n",
    "    try:\n",
    "        return freq_df.loc[t0:t1]['freq'][0]\n",
    "    except:\n",
    "        # return last frequency you found\n",
    "        return freq_df.iloc[-1]['freq']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Rebuild database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load data from AWS SQL database \n",
    "\n",
    "(necessary only to load Muni routes and estlabish connection to database)\n",
    "\n",
    "We need to connect to the PostgresSQL database that I am reading the NextBus Muni data into, which is called 'sf_muni_arrivals' in our case.\n",
    "\n",
    "**This cell must be run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MUST BE RUNNING \n",
    "# ssh -i ~/.ssh/aws_instance_3_instantiated_2016_02_03.pem -L 63333:127.0.0.1:5432 ec2-user@ec2-52-72-119-113.compute-1.amazonaws.com\n",
    "# for this to work (for that specific AWS instance)\n",
    "remote_dbname = 'sf_muni_arrivals_aws'\n",
    "remote_username = 'ec2-user'\n",
    "remote_table = 'nextbus_realtime_with_predictions'\n",
    "\n",
    "# Open up an engine, that we will use to create the database if it doesn't exist\n",
    "engine = create_engine('postgres://%s@localhost:63333/%s'%(remote_username,remote_dbname))\n",
    "\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "    \n",
    "# If I want to filter the data first:\n",
    "# connect:\n",
    "db_con = None\n",
    "db_con = psycopg2.connect(database = remote_dbname, user = remote_username, port = 63333, host = 'localhost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that connection is established, load database into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql_query = '''SELECT * FROM {table};'''.format(table=remote_table)\n",
    "df_all = pd.read_sql_query(sql_query, db_con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Build 'freq' and 'normed_pred_diff' columns\n",
    "\n",
    "'freq' is the frequency of the given route at that time of day. For example, during peak hours, the route 30 has 'freq' of 5.0 (it runs every 5 minute).\n",
    "\n",
    "'normed_pred_diff' is the difference in predictions time compared to the scheduled difference time. For example, if a route runs every 10 minutes and the predictions for the next two vehicles are 2 and 12 (in minutes), 'normed_pred_diff' is abs((2-12)/10) = 1.0. If the predictions were instead 2 and 7, we'd have abs((2-7)/10) = 0.5, suggesting a more bunched bus (note, however, in the final analysis we define a different parameter that time averages this and maps entirely bunched buses (here these would be 'normed_pred_diff' = 0.0) to 1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just for cleaning\n",
    "df_tmp = df_all.drop(['index'], axis=1)\n",
    "# build 'freq' column\n",
    "df_tmp['freq'] = df_tmp.apply(lambda row: get_frequency_for_route(row['route_x'],row['time']), axis=1)\n",
    "# build 'normed_pred_diff' column\n",
    "df_tmp['normed_pred_diff'] = df_tmp.apply(lambda row: \n",
    "                                          (np.abs(float(row['pred_x'])-float(row['pred_y'])))/float(row['freq']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Write to database\n",
    "Now write this back into the database, so we can analyze it later. Use the psychogp2 engine we defined above (this writes back into the database we already connected to). We specify the table as the first argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tmp.to_sql('nextbus_realtime_with_predictions_and_freqs', engine, if_exists='append')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
