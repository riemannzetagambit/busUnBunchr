{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load up our libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all purpose\n",
    "import datetime, geoplotlib, re\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "# for talking to SQL databases\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "\n",
    "# json and XML parsing\n",
    "import json\n",
    "from pprint import pprint\n",
    "from urllib2 import urlopen\n",
    "from pyquery import PyQuery as pq\n",
    "\n",
    "# for making maps\n",
    "import geoplotlib\n",
    "from geoplotlib.utils import BoundingBox\n",
    "from IPython.display import Image\n",
    "\n",
    "# all purpose data analysis and plotting\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define this function for computing distances in meters from (lat,lon) coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will need this function to compute the distance between two (lat,lon) points, in meters\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    meters = 6367 * c * 1000\n",
    "    return meters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from SQL database (necessary only to load Muni routes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to connect to the PostgresSQL database that I am reading the NextBus Muni data into, which is called 'sf_muni_arrivals' in our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbname = 'sf_muni_arrivals'\n",
    "username = 'dstone'\n",
    "table = 'nextbus_write_2016_01_15'\n",
    "\n",
    "# Open up an engine, that we will use to create the database if it doesn't exist\n",
    "engine = create_engine('postgres://%s@localhost/%s'%(username,dbname))\n",
    "\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The following command loads the ENTIRE SQL database from above into a pandas dataframe. In the future, I will want to sort the data first with SQL commands, then load it into a pandas dataframe. I can do that with the commented out text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If I want to filter the data first:\n",
    "# connect:\n",
    "db_con = None\n",
    "db_con = psycopg2.connect(database = dbname, user = username)\n",
    "# the table name is 'nextbus':\n",
    "# only run this code if the list_of_muni_routes needs to be reestablished\n",
    "# sql_query = \"\"\"\n",
    "# SELECT * FROM {table};\n",
    "# \"\"\".format(table=table)\n",
    "# nbdata = pd.read_sql_query(sql_query,db_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# routeslist = pd.unique(nbdata['route'].ravel())\n",
    "# list_of_muni_routes = pd.DataFrame(data=routeslist, index = range(len(routeslist)), columns=['route'])\n",
    "\n",
    "# # Clean up a bit\n",
    "# list_of_muni_routes = list_of_muni_routes[(list_of_muni_routes.route.isnull() == False) & (list_of_muni_routes.route != 'Inspectors') & (list_of_muni_routes.route != 'Training')]\n",
    "# list_of_muni_routes = list_of_muni_routes[list_of_muni_routes.route != '']\n",
    "# np.save('list_of_muni_routes',np.asarray(list_of_muni_routes).ravel())\n",
    "list_of_muni_routes = np.load('list_of_muni_routes.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute distance distributions for each route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All documentation for this function can be found in the exploration I did above for route 30 specifically\n",
    "def write_dists_for_all_routes_to_disk():\n",
    "    def compute_dists_for_route(route_num, database_connection, table_to_read):\n",
    "        sql_query = '''SELECT * FROM {table} WHERE route = \\'{route}\\';'''.format(table=table_to_read,route=route_num)\n",
    "        \n",
    "        route_dat = pd.read_sql_query(sql_query, database_connection)\n",
    "        \n",
    "        # Need to coarse-grain the time\n",
    "        route_dat.time = route_dat.apply(lambda row: compute_datetime(row['time']), axis=1)\n",
    "\n",
    "        trips_in_order_for_route = np.sort(pd.unique(route_dat[route_dat.trip != ''].trip.ravel()))\n",
    "\n",
    "        coords_dict = {}\n",
    "        for i in range(len(trips_in_order_for_route)):\n",
    "            my_key = \"trip_\"+str(i)\n",
    "            if my_key not in coords_dict:\n",
    "                coords_dict[my_key] = 0\n",
    "\n",
    "        for i in range(len(trips_in_order_for_route)):\n",
    "            trip_temp = 'trip_'+str(i)\n",
    "            tmp_vehicles = pd.unique(route_dat[(route_dat.trip == trips_in_order_for_route[i])].vehicle.ravel())\n",
    "            coords_dict[trip_temp] = route_dat[(route_dat.trip == trips_in_order_for_route[i]) & (route_dat.vehicle == tmp_vehicles[0])][['lat','lon','time']]\n",
    "            coords_dict[trip_temp].drop_duplicates(subset='time', inplace=True)\n",
    "            coords_dict[trip_temp].index = coords_dict[trip_temp].time\n",
    "\n",
    "        dist_tmp = pd.DataFrame()\n",
    "\n",
    "        for i in range(len(trips_in_order_for_route)-1):\n",
    "            trip_now = 'trip_'+str(i)\n",
    "            trip_next = 'trip_'+str(i+1)\n",
    "            #print 'Merging {trip1} and {trip2}'.format(trip1=trip_now,trip2=trip_next)\n",
    "            tmp = pd.merge(left=coords_dict[trip_now],right=coords_dict[trip_next],on='time')\n",
    "            if tmp.shape[0] != 0:\n",
    "                tmp['dist'] = tmp.apply(lambda row: haversine(row['lat_x'],row['lon_x'],row['lat_y'],row['lon_y']), axis=1)\n",
    "                dist_tmp = pd.concat([dist_tmp, tmp[['time','dist']]])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        dist_tmp.index = range(len(dist_tmp))\n",
    "\n",
    "        return dist_tmp\n",
    "    \n",
    "    route_hist_dict={}\n",
    "    for i in list_of_muni_routes:\n",
    "            my_key = 'route_'+str(i)\n",
    "            if my_key not in route_hist_dict:\n",
    "                route_hist_dict[my_key] = 0\n",
    "    #build a dict here, then fill in each element of the dict \n",
    "    #   with the corresponding 'compute_dists_for_route' so \n",
    "    #   we have everything ready to go for each route\n",
    "    for route_num in list_of_muni_routes:\n",
    "        temp_Route = 'route_'+str(route_num)\n",
    "        route_hist_dict[temp_Route] = compute_dists_for_route(route_num, db_con, 'nextbus_write_2016_01_15')\n",
    "        if len(route_hist_dict[temp_Route]) != 0:\n",
    "            path_to_file = 'muni_route_distance_distributions/'+temp_Route+'_distribution'\n",
    "            np.save(path_to_file,np.asarray(route_hist_dict[temp_Route]['dist']).ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each route in the Muni list, write the array of distances between pairs of buses to disk for reading later from the directory './muni_route_distance_distributions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_dists_for_all_routes_to_disk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write distance pairs to database with generate_route_pair function \n",
    "Pulls data from SQL database, computes entry in final database, which is pairs subsequent buses on the same route\n",
    "\n",
    "Now we write the function here. Idea: run over 'list_of_muni_routes', for each route, create the pairs (there will be many) of entries (this is done in various sections of code below), write them to a SQL database, db and table of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To convert the timestamps from SQL database to datetime objects\n",
    "def compute_datetime(timestamp):\n",
    "    return datetime.datetime(pd.to_datetime(timestamp).year, \n",
    "                   pd.to_datetime(timestamp).month, \n",
    "                   pd.to_datetime(timestamp).day, \n",
    "                   pd.to_datetime(timestamp).hour, \n",
    "                   pd.to_datetime(timestamp).minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function generates a table of pairs for a given route\n",
    "# assume a database connection is already established\n",
    "def generate_route_pairs(route, psycopg2_engine, database_connection, table_to_read, table_to_write):\n",
    "    '''route: Muni route you want to generate data for\n",
    "    psycopg2_engine: engine (postgreSQL) you wish to use to write data to\n",
    "    database_connection: database that engine provides connection to where you will write\n",
    "    table_to_read: grab route data from this table\n",
    "    table_to_write: write bus pair data to this table'''\n",
    "    sql_query = '''SELECT * FROM {table} WHERE route = \\'{route}\\';'''.format(table=table_to_read,route=route)\n",
    "    df_route = pd.read_sql_query(sql_query, database_connection)\n",
    "    \n",
    "    # Need to coarse-grain the time\n",
    "    df_route.time = df_route.apply(lambda row: compute_datetime(row['time']), axis=1)\n",
    "    \n",
    "    route_trips = np.sort(pd.unique(df_route[df_route.trip != ''].trip.ravel()))\n",
    "   \n",
    "    coords_dict = {}\n",
    "    for i in range(len(route_trips)):\n",
    "        my_key = \"trip_\"+str(i)\n",
    "        if my_key not in coords_dict:\n",
    "            coords_dict[my_key] = 0\n",
    "            \n",
    "    # some editing to be done here\n",
    "    for i in range(len(route_trips)):\n",
    "        trip_temp = 'trip_'+str(i)\n",
    "        tmp_vehicles = pd.unique(df_route[(df_route.trip == route_trips[i])].vehicle.ravel())\n",
    "        coords_dict[trip_temp] = df_route[(df_route.trip == route_trips[i]) & (df_route.vehicle == tmp_vehicles[0])]\n",
    "        coords_dict[trip_temp].drop_duplicates(subset='time', inplace=True)\n",
    "        coords_dict[trip_temp].index = coords_dict[trip_temp].time\n",
    "        \n",
    "    \n",
    "    #dist_tmp = pd.DataFrame()\n",
    "    \n",
    "    # Compute the distance between each\n",
    "    def compute_distance_percentile(route_num, lat1, lon1, lat2, lon2):\n",
    "        tmp_dist = haversine(lat1,lon1,lat2,lon2)\n",
    "        path_to_dist_distribution = 'muni_route_distance_distributions/route_'+str(route_num)+'_distribution.npy'\n",
    "        route_dist_distribution = np.load(path_to_dist_distribution)\n",
    "        percentile_score = 1 - stats.percentileofscore(route_dist_distribution, tmp_dist)/100\n",
    "        return percentile_score\n",
    "    \n",
    "    for i in range(len(route_trips)-1):\n",
    "        trip_now = 'trip_'+str(i)\n",
    "        trip_next = 'trip_'+str(i+1)\n",
    "        #print 'Merging {trip1} and {trip2}'.format(trip1=trip_now,trip2=trip_next)\n",
    "        tmp = pd.merge(left=coords_dict[trip_now],right=coords_dict[trip_next],on='time')\n",
    "        if tmp.shape[0] != 0:\n",
    "            tmp['dist_percentile'] = tmp.apply(lambda row: compute_distance_percentile(route, row['lat_x'],row['lon_x'],row['lat_y'],row['lon_y']), axis=1)\n",
    "            # Write this tuple to the SQL table\n",
    "            tmp.to_sql(table_to_write, psycopg2_engine, if_exists='append')\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each route, add to the 'final' database an entry for each pair in that route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for route_num in list_of_muni_routes:\n",
    "    generate_route_pairs(str(route_num), engine, db_con, 'nextbus_write_2016_01_15', 'vehicle_pairs_2016_01_15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data from Google Maps API\n",
    "\n",
    "From there get stop ID of starting bus, from there get bus data from NextBus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('google_maps_api/second_dirs.json') as data_file:\n",
    "    data = json.load(data_file)\n",
    "\n",
    "#pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "route_name = str(data['routes'][0]['legs'][0]['steps'][0]['transit_details']['line']['short_name'])\n",
    "departure_stop = str(data['routes'][0]['legs'][0]['steps'][0]['transit_details']['departure_stop']['name'])\n",
    "departure_lat = float(data['routes'][0]['legs'][0]['steps'][0]['transit_details']['departure_stop']['location']['lat'])\n",
    "departure_lon = float(data['routes'][0]['legs'][0]['steps'][0]['transit_details']['departure_stop']['location']['lng'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "26th St & De Haro St\n",
      "37.75086\n",
      "-122.40015\n"
     ]
    }
   ],
   "source": [
    "print route_name\n",
    "print departure_stop\n",
    "print departure_lat\n",
    "print departure_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_get_route_config='http://webservices.nextbus.com/service/publicXMLFeed?command=routeConfig&a=sf-muni&r='+str(route_name)\n",
    "route_config = pq(urlopen(url_get_route_config).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match at 26th St & De Haro St with stop id: 13516\n",
      "Coordinates of stop: (37.75086,-122.40015)\n"
     ]
    }
   ],
   "source": [
    "for bus_stop_obj in route_config('stop'):\n",
    "    bus_stop = pq(bus_stop_obj)\n",
    "    if bus_stop.attr('lat') is not None:\n",
    "        stop_name = str(bus_stop.attr('title'))\n",
    "        stop_lat = round(float(bus_stop.attr('lat')),5)\n",
    "        stop_lon = round(float(bus_stop.attr('lon')),5)\n",
    "        if stop_name == departure_stop and stop_lat == departure_lat and stop_lon == departure_lon:\n",
    "            stop_id = str(bus_stop.attr('stopId'))\n",
    "            print 'Match at '+stop_name+' with stop id: '+stop_id\n",
    "            print 'Coordinates of stop: ('+str(stop_lat)+','+str(stop_lon)+')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_get_stop_info='http://webservices.nextbus.com/service/publicXMLFeed?command=predictions&a=sf-muni&stopId='+stop_id\n",
    "stop_config = pq(urlopen(url_get_stop_info).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found correct bus route 19, trip ID = None\n"
     ]
    }
   ],
   "source": [
    "for next_bus_obj in stop_config('predictions'):\n",
    "    if str(pq(next_bus_obj).attr('routeTag')) == route_name:\n",
    "        for upcoming_trips in pq(next_bus_obj)\n",
    "        trip_id = pq(next_bus_obj).attr('tripTag')\n",
    "        print 'Found correct bus route '+str(pq(next_bus_obj).attr('routeTag'))+', trip ID = '+ str(trip_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!head nextbus_one_day_sf_muni_dump.csv > tmp_dump.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('tmp_dump.txt') as my_file:\n",
    "    data = my_file.readlines()\n",
    "my_file.close()\n",
    "\n",
    "for i, line in enumerate(data,0):\n",
    "    # get rid of whitespaces, parentheses in coordinates, replace comma in coordinates with '|' so it splits,\n",
    "    # get rid of newlines, split on '|'\n",
    "    data[i] = line.replace(',','|').replace(' ','').replace('(','').replace(')','').replace('\\n','').split('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vehiclid', 'receivedtimestamp', 'gpstimestamp', 'gpsfix', 'gpsspeed', 'gpsheading', 'route', 'trip']\n",
      "2016-01-1400:00:00.099-08\n"
     ]
    }
   ],
   "source": [
    "print data[0]\n",
    "print data[3][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build distance distributions (nightly), save to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate database of pairs\n",
    "With distance quantiles built in from previous function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
